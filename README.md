# Fineâ€‘Tuning **ResNetâ€‘18** on **Caltechâ€‘101**

> **Transferâ€‘learning beats training from scratch by **â‰ˆâ€¯+22â€¯pp** on a 30â€‘shot Caltechâ€‘101 split.**

This repository contains the full PyTorch implementation, training scripts and utilities for the experiments described in the report:

> *HuangÂ Jichuan & ZhaoÂ Junming â€“ â€œFineâ€‘tuning Preâ€‘trained Convolutional Neural Networks for Caltechâ€‘101 Image Classification: A Comparative Studyâ€*
> *(see `report.tex`)*

---

## ğŸ“‚ Directory Layout

```
caltech101_finetuning/
â”œâ”€â”€ data_utils.py          # dataset download, split & transforms
â”œâ”€â”€ model_utils.py         # model building & differential LR helper
â”œâ”€â”€ train_caltech101.py    # training / evaluation loop
â”œâ”€â”€ download.py            # oneâ€‘click Caltechâ€‘101 fetch
â”œâ”€â”€ checkpoints/           # *.pth weights & TensorBoard logs
â”‚Â Â  â”œâ”€â”€ r18_pretrain_lowwd_mix02/  # RunÂ 5 (best) â€“ 0.940 Valâ€‘Acc
â”‚Â Â  â””â”€â”€ ...
â”œâ”€â”€ requirements.txt       # exact Python package versions
â””â”€â”€ README.md              # you are here
```

Feel free to rename the root folder; paths are handled via arguments.

---

## âš™ï¸  Quick Start

```bash
# 1. Create & activate a Python â‰¥3.9 environment
conda create -n caltech101 python=3.9 -y
conda activate caltech101

# 2. Install dependencies
pip install -r requirements.txt

# 3. Download Caltechâ€‘101 (~130â€¯MB) and split 30/valâ€‘rest
python download.py  # creates data/Caltech101/

# 4. Fineâ€‘tune the preâ€‘trained ResNetâ€‘18 (RunÂ 3 by default)
python train_caltech101.py \
    --exp r18_pretrain_main \
    --data_root data/Caltech101/ \
    --output_dir checkpoints/
```

TensorBoard logs are written under `checkpoints/<run_name>/tb/`. Start TensorBoard via `tensorboard --logdir checkpoints/`.

---

## ğŸ‹ï¸â€â™€ï¸  Reproducing the Published Runs

| Run   | Scratch | Epochs | Batch |  LRÂ BB |  LRâ€¯FC |  WD  | MixUpÂ Î± | CutMixÂ Î± |  Valâ€‘Acc  |
| ----- | ------- | :----: | :---: | :----: | :----: | :--: | :-----: | :------: | :-------: |
| **1** | âœ”       |   300  |   64  |  3eâ€‘3  |  3eâ€‘3  | 1eâ€‘3 |   0.4   |    1.0   |   0.712   |
| **2** | âœ”       |   300  |   64  | 2.5eâ€‘3 | 2.5eâ€‘3 | 2eâ€‘3 |   0.4   |    1.0   |   0.723   |
| **3** | âœ–       |   200  |  128  |  2eâ€‘4  |  5eâ€‘3  | 1eâ€‘4 |   0.4   |    1.0   |   0.932   |
| **4** | âœ–       |   200  |  128  |  3eâ€‘4  |  5eâ€‘3  | 1eâ€‘4 |   0.4   |    1.0   |   0.935   |
| **5** | âœ–       |   240  |   96  |  2eâ€‘4  |  4eâ€‘3  | 5eâ€‘5 |   0.2   |    1.0   | **0.940** |

Use the exact preset names found in `train_caltech101.py`:

```bash
python train_caltech101.py --exp r18_pretrain_lowwd_mix02  # RunÂ 5
```

Preâ€‘trained ImageNet weights are automatically downloaded by `torchvision` on first run.

---

## ğŸ’¾  Download Trained Weights

```bash
# Best fineâ€‘tuned model (RunÂ 5)
wget -O resnet18_caltech101_run5.pth \
  "https://drive.google.com/uc?export=download&id=1cu5nu9MQMsNCxg1yRiNeK9cqiHB2wR8c"
```

Load in Python:

```python
import torch, model_utils
model = model_utils.get_model(num_classes=101, pretrained=False)
state_dict = torch.load('resnet18_caltech101_run5.pth', map_location='cpu')
model.load_state_dict(state_dict)
model.eval()
```

---

## ğŸ”§  Key Training Flags

| Flag                               | Purpose                                    | Default            |
| ---------------------------------- | ------------------------------------------ | ------------------ |
| `--scratch`                        | Train from random init instead of ImageNet | `False`            |
| `--epochs`                         | Training epochs                            | *presetâ€‘dependent* |
| `--batch_size`                     | Global batch size                          | *presetâ€‘dependent* |
| `--lr_backbone` / `--lr_fc`        | Differential learning rates                | â€”                  |
| `--weight_decay`                   | L2 WD                                      | â€”                  |
| `--mixup_alpha` / `--cutmix_alpha` | Augmentation strengths                     | â€”                  |
| `--img_size`                       | Input resolution                           | 224                |
| `--workers`                        | Dataâ€‘loader processes                      | 8                  |

Run `python train_caltech101.py --help` for all options.

---

## ğŸ“ˆ  Expected GPU Utilisation

| GPU            | Batch 128 (FP32) | Peak VRAM |
| -------------- | ---------------- | --------- |
| RTXÂ 3060Â 12â€¯GB | âœ“                | \~7â€¯GB    |
| RTXÂ 2080Â 8â€¯GB  | âœ“ (batchÂ 96)     | \~7â€¯GB    |

Training time for RunÂ 5 on a single RTXÂ 3060 â‰ˆ **2â€¯h**.

---

## ğŸ“  Citation

If you use this code or trained weights in your research, please cite the accompanying report:

```
@report{huang2025caltech101,
  title   = {Fineâ€‘tuning Preâ€‘trained Convolutional Neural Networks for Caltechâ€‘101 Image Classification: A Comparative Study},
  author  = {Huang, Jichuan and Zhao, Junming},
  year    = {2025},
  note    = {Technical report},
}
```

---

## ğŸ—’ï¸  License

Distributed under the **MIT License** â€“ see `LICENSE` for details.

---

## ğŸ™  Acknowledgements

* Caltech for releasing the Caltechâ€‘101 dataset.
* The PyTorch & Torchvision teams.
* Original ImageNet preâ€‘training courtesy of \[PyTorch Hub].

---

Happy fineâ€‘tuning! ğŸš€
